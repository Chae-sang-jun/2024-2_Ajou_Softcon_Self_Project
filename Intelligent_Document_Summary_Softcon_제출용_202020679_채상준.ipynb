{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chae-sang-jun/2024-2_Ajou_Softcon_Self_Project/blob/main/Intelligent_Document_Summary_Softcon_%EC%A0%9C%EC%B6%9C%EC%9A%A9_202020679_%EC%B1%84%EC%83%81%EC%A4%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K-G3LiES7-xW",
        "outputId": "0d95f2c6-fc0c-4d14-d4f8-a3fdb5df62b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-79jz2z3f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-79jz2z3f\n",
            "  Resolved https://github.com/openai/whisper.git to commit 173ff7dd1d9fb1c4fddea0d41d704cfefeb8908c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gradio==4.44.1 in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.115.5)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.26.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.8.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.0.17)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.8.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.1) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio==4.44.1) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio==4.44.1) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.1) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.1) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio==4.44.1) (0.41.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.44.1) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.44.1) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.1) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.1) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.44.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.44.1) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.44.1) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.1) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://030d81386297c6362a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://030d81386297c6362a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://030d81386297c6362a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Install required dependencies\n",
        "!pip install gradio==4.44.1 transformers openai deep-translator langdetect PyPDF2 git+https://github.com/openai/whisper.git\n",
        "\n",
        "# Import libraries\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "from deep_translator import GoogleTranslator\n",
        "from langdetect import detect\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Initialize tools\n",
        "translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
        "translator_ko = GoogleTranslator(source=\"auto\", target=\"ko\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "qa_model = pipeline(\"question-answering\")  # Hugging Face QA pipeline\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Utility Functions\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess the text.\"\"\"\n",
        "    return text.strip()\n",
        "\n",
        "def save_to_file(content, filename):\n",
        "    \"\"\"Save the content to a file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "        return f\"File saved as {filename}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error saving file: {e}\"\n",
        "\n",
        "def split_text(text, max_length):\n",
        "    \"\"\"Split text into chunks of a maximum token length.\"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text[:max_length].rfind(\". \")  # Split at the nearest sentence boundary\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "    chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def summarize_long_text(text):\n",
        "    \"\"\"Summarize text that exceeds the model's maximum input length.\"\"\"\n",
        "    max_input_length = 1024  # BART's max token length\n",
        "    text_chunks = split_text(text, max_input_length)\n",
        "    summarized_chunks = [summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0][\"summary_text\"] for chunk in text_chunks]\n",
        "    return \" \".join(summarized_chunks)\n",
        "\n",
        "def extract_text_from_pdf(file):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file.name)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return preprocess_text(text)\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF file: {e}\"\n",
        "\n",
        "def text_summarization_workflow(file):\n",
        "    \"\"\"Summarize text from the uploaded file.\"\"\"\n",
        "    try:\n",
        "        if file.name.endswith(\".pdf\"):\n",
        "            # Handle PDF files\n",
        "            original_text = extract_text_from_pdf(file)\n",
        "        else:\n",
        "            # Handle plain text files\n",
        "            with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
        "                original_text = f.read()\n",
        "\n",
        "        original_text = preprocess_text(original_text)\n",
        "        summarized_text = summarize_long_text(original_text)\n",
        "        return original_text, summarized_text, \"\"\n",
        "    except Exception as e:\n",
        "        return \"\", \"\", f\"Error processing text: {e}\"\n",
        "\n",
        "def stt_summarization_workflow(audio):\n",
        "    \"\"\"Transcribe and summarize text from the uploaded audio file.\"\"\"\n",
        "    try:\n",
        "        # Transcribe audio using Whisper\n",
        "        transcription_result = whisper_model.transcribe(audio)\n",
        "        stt_text = transcription_result[\"text\"]\n",
        "\n",
        "        # Detect language of the transcribed text\n",
        "        detected_lang = detect(stt_text)\n",
        "\n",
        "        if detected_lang == \"ko\":\n",
        "            # Translate Korean text to English\n",
        "            stt_text_en = translator.translate(stt_text, target=\"en\")\n",
        "\n",
        "            # Summarize the translated English text\n",
        "            summarized_text_en = summarize_long_text(stt_text_en)\n",
        "\n",
        "            # Translate the summarized text back to Korean\n",
        "            summarized_text_ko = translator_ko.translate(summarized_text_en, target=\"ko\")\n",
        "            return stt_text, summarized_text_ko, \"\"\n",
        "        else:\n",
        "            # Directly summarize if the text is in English\n",
        "            summarized_text = summarize_long_text(stt_text)\n",
        "            return stt_text, summarized_text, \"\"\n",
        "    except Exception as e:\n",
        "        return \"\", \"\", f\"Error processing audio: {e}\"\n",
        "\n",
        "def generate_chat_response(context, question):\n",
        "    \"\"\"Generate a chatbot response using Hugging Face QA pipeline.\"\"\"\n",
        "    try:\n",
        "        # Detect the question language\n",
        "        lang = detect(question)\n",
        "\n",
        "        if lang == \"ko\":\n",
        "            # If the question is in Korean, translate to English\n",
        "            question_en = translator.translate(question, target=\"en\")\n",
        "            # Generate the response in English\n",
        "            response = qa_model(question=question_en, context=context)\n",
        "            # Translate the response back to Korean\n",
        "            response_ko = translator_ko.translate(response['answer'], target=\"ko\")\n",
        "            return response_ko\n",
        "        else:\n",
        "            # If the question is in English, process directly\n",
        "            response = qa_model(question=question, context=context)\n",
        "            return response['answer']\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {e}\"\n",
        "\n",
        "# Gradio Interface\n",
        "def create_interface():\n",
        "    with gr.Blocks() as app:\n",
        "        # Initial Screen\n",
        "        with gr.Row():\n",
        "            gr.Markdown(\"## Welcome! Choose a Workflow:\")\n",
        "            btn_text_summary = gr.Button(\"Text Summarization\")\n",
        "            btn_stt_summary = gr.Button(\"Speech to Text + Summarization\")\n",
        "\n",
        "        # Text Summarization Workflow\n",
        "        with gr.Row(visible=False) as text_summary_section:\n",
        "            gr.Markdown(\"### Text Summarization Workflow\")\n",
        "            text_file = gr.File(label=\"Upload Text File (PDF or TXT)\")\n",
        "            original_text_display = gr.Textbox(label=\"Original Text\", lines=10)\n",
        "            summarized_text_display = gr.Textbox(label=\"Summarized Text\", lines=10)\n",
        "            btn_save_summary = gr.Button(\"Save Summarized Text\")\n",
        "            btn_proceed_chatbot = gr.Button(\"Proceed to Chatbot\")\n",
        "\n",
        "        # Speech-to-Text and Summarization Workflow\n",
        "        with gr.Row(visible=False) as stt_summary_section:\n",
        "            gr.Markdown(\"### Speech to Text + Summarization Workflow\")\n",
        "            audio_file = gr.Audio(label=\"Upload Audio File\", type=\"filepath\")\n",
        "            stt_text_display = gr.Textbox(label=\"Transcribed Text\", lines=10)\n",
        "            summarized_text_display_stt = gr.Textbox(label=\"Summarized Text\", lines=10)\n",
        "            btn_save_stt = gr.Button(\"Save Transcribed Text\")\n",
        "            btn_save_summary_stt = gr.Button(\"Save Summarized Text\")\n",
        "            btn_proceed_chatbot_stt = gr.Button(\"Proceed to Chatbot\")\n",
        "\n",
        "        # Chatbot Section\n",
        "        with gr.Row(visible=False) as chatbot_section:\n",
        "            gr.Markdown(\"### Chatbot Interaction (한국어/영어 지원)\")\n",
        "            chatbot_context_display = gr.Textbox(label=\"Chatbot Context\", lines=10, interactive=False)\n",
        "            chatbot_user_input = gr.Textbox(label=\"Ask a Question (English or Korean)\", placeholder=\"Enter your question...\")\n",
        "            chatbot_response_display = gr.Textbox(label=\"Chatbot Response\", lines=5)\n",
        "            btn_chatbot_submit = gr.Button(\"Submit Question\")\n",
        "\n",
        "        # Navigation Logic\n",
        "        def show_text_summary():\n",
        "            return gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        def show_stt_summary():\n",
        "            return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)\n",
        "\n",
        "        def show_chatbot(context):\n",
        "            return gr.update(visible=False), gr.update(visible=False), gr.update(visible=True), context\n",
        "\n",
        "        btn_text_summary.click(show_text_summary, outputs=[text_summary_section, stt_summary_section, chatbot_section])\n",
        "        btn_stt_summary.click(show_stt_summary, outputs=[text_summary_section, stt_summary_section, chatbot_section])\n",
        "        btn_proceed_chatbot.click(\n",
        "            lambda text: show_chatbot(text),\n",
        "            inputs=[summarized_text_display],\n",
        "            outputs=[text_summary_section, stt_summary_section, chatbot_section, chatbot_context_display],\n",
        "        )\n",
        "        btn_proceed_chatbot_stt.click(\n",
        "            lambda text: show_chatbot(text),\n",
        "            inputs=[summarized_text_display_stt],\n",
        "            outputs=[text_summary_section, stt_summary_section, chatbot_section, chatbot_context_display],\n",
        "        )\n",
        "\n",
        "        # Workflow Logic\n",
        "        text_file.change(\n",
        "            text_summarization_workflow,\n",
        "            inputs=[text_file],\n",
        "            outputs=[original_text_display, summarized_text_display],\n",
        "        )\n",
        "        btn_save_summary.click(lambda text: save_to_file(text, \"summary.txt\"), inputs=[summarized_text_display], outputs=[])\n",
        "\n",
        "        audio_file.change(\n",
        "            stt_summarization_workflow,\n",
        "            inputs=[audio_file],\n",
        "            outputs=[stt_text_display, summarized_text_display_stt],\n",
        "        )\n",
        "        btn_save_stt.click(lambda text: save_to_file(text, \"stt_text.txt\"), inputs=[stt_text_display], outputs=[])\n",
        "        btn_save_summary_stt.click(lambda text: save_to_file(text, \"summarized_stt.txt\"), inputs=[summarized_text_display_stt], outputs=[])\n",
        "\n",
        "        btn_chatbot_submit.click(\n",
        "            generate_chat_response,\n",
        "            inputs=[chatbot_context_display, chatbot_user_input],\n",
        "            outputs=[chatbot_response_display],\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Launch the application\n",
        "create_interface().launch(share=True, debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6dc4yjz83TRGPIBRnQTDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}